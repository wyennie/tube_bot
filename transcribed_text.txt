This video is going to be a step-by-step guide on how you can fine-tune your own ChatGPT 3.5 turbo model. We are going to go over how to prepare your data sets, how to create the fine-tuning model, how to use the fine-tuning model, and some stuff around pricing, pros and cons on fine-tuning. So let's just get going. Let's just start by looking at why you even would consider fine-tuning a model. So we're just going to go to the OpenAI website here and look at some fine-tuning use cases, so you can see what they have listed here is improved durability, reliable output formatting, as you will see in my example, this is kind of my bread and butter here, and we can set a custom tone, that is very easy to do too, so this is like a system prompt baked into your model already. Another great benefit is you can shorten your prompts, so let's say you have a very long prompt you always use to run like in your application or something, it can be something else too. If you fine-tune on that prompt, you can basically remove that and get more tokens. So you can see early testers have reduced prompt size up to 90% by fine-tuning instructions into the model itself, speeding up the API call and cutting costs. So this means like if you have a very big input prompt, this will save you money as well as time right on inference. So there are some good use cases for fine-tuning, but it's not for everyone, and hopefully this guide will kind of give you an idea what fine-tuning can be good for. The first step of fine-tuning our model is going to be to prepare our data set. So here we have the fine-tuning steps. Step one, prepare your data. This is the format we want every example we are going to send in to fine-tune our model to be in. It's a JSON setup here with three different inputs. We have the system prompt or role. So yeah, you can see this is like our system prompt. This is the user or this is our prompt, right? And we have the response from the model. We're going to go through this in a bit more detail, but I just want to show you this. And this is the data set I trained my model on. So this fine-tuned model is going to be basically an AI story Instagram fine-tuned model. So how I did this, I kind of created the first one here over on GPT-4. I thought it's a very simple way to create this. So you can see here is my fine-tuning example. So we have a system role prompt. So here I just filled in like my system prompt. If we go to the playground here, you can see we have this system here, and you can see this is basically what we are filling in in this spot over here, right? So this is our system role. The prompt is going to be, so you can see user prompt, right? Very intriguing short conspiracy mystery stories, 60 to 90 seconds about a given topic for short form videos. And then I assign a topic here, right? Mystery, numerology and a sudden change in personality. And of course, when I assign this, I also need to put in what kind of response I want back from this. And this is where I go to my data set, I find the title here for this kind of story, right? So I think it was. Yeah, I think it was this one, right? And so what we do, then we just copy the title, we go back here. And we want the response to be in this format. So we want the title, we want the title, we want for this story. And then we're going to paste in our story too, right? So the mysterious case of Hannah Collins, and we're going to copy the story here, right? And we paste that in down here. Okay, so what we have, then is kind of we have a system prompt, we have the user prompt, this is our input. And we have the response we want. And then I go ahead and this is the example format, I want this in. So this is basically going to be this one, right? And I just use GPT for to create this for me. And then we can see we have a JSON object here. And this is the format. So you can see the system role is you are a creative exceptional writer, you write short. So basically the same as we have over here. And we have our input prompt. So that is the user right over an intriguing short conspiracy mystery stories. So this is the user input, right? And we have the response. So we have the title, and we have the story. So title, and story, right? And there is our first example. So what I do then is I just copy this JSON here, just go to an empty text file, and I just paste it in here. So this is our first example, right? So example one. So you can see we have the system role, and we have the input, we have the user input. So this is our prompt, and we have the response, right? So there we have one of the examples we need to fine-tune this model. So how many examples do you actually need to fine-tune your model? If we look at the OpenAI documentation here, so you can see to fine-tune our model, you are required to provide at least 10 examples. But we typically see clear improvements from fine-tuning to on 50 to 100 training examples with GPT 3.5 Turbo. But the right number varies greatly based on the exact use case. So in our case, it's just gonna be, I think it's 18 or 19 examples. That's very low. But as you will see, for me, that worked quite well. Actually, I was quite surprised by that. But in some cases, you might need like 1000 examples, or maybe 500. It's kind of hard to tell. But this is going to be a very simple test, so how you scale this is kind of up to you. The next step then is going to be to collect every single example I need from my dataset. So what I went ahead and did, I just kind of run this a few times. This was a very small dataset, so I just did it kind of manually. You can do this with Python script if you kind of want a bigger dataset, then it's not so good to do that manually. But basically, like I showed you, we collected all of these examples in this, just this text file here. And I went to notepad here and just pasted that into this notepad here. I went ahead and saved this as a JSON object, right? And then we get this kind of structure here. So here is basically all our 16 or 17 examples. But we want this in a kind of a different format for the fine-tuning job. So if we go back here to GPT-4, I'm going to show you a prompt I used for this. You can see here I have uploaded the JSON file with examples. So here is basically I've uploaded the JSON file we have here. And I went, can you turn that into a separated JSON L formatted conversation using these three steps? Split the content based on the pattern scene indicator start conversation, parse each split selection, and if successful, add it to a list of conversations. And I just press submit on that. And it kind of went through this workload here. And I saved the conversation in a JSON L format. You can download it from here. Perfect. So I went ahead, downloaded that. And here is kind of the final form we want our dataset to be in. So you can see we have a numbered here. And this is the first example, right? So you can see I have a total of 16 examples here. So remember, we need a minimum 10 examples to do a fine-tuning job. I have 16 now. You can have as many as you want, of course, but the price is going to go up the more examples you have. But this means now that we are ready to create a fine-tuning job. And that brings us to step number two. And that is going to be uploading our examples to OpenAI. So you can see in our fine-tuning steps here, step two, upload files. For this, the best thing is to use a small Python script that I have created here. If you are interested in this Python script, if you just want to copy it from my GitHub, you can find a link to my membership down below where I'm going to do more tutorials on fine-tuning and stuff. And I'm going to be uploading all of these scripts to the community GitHub over there. You can just download them and use this, or you can just copy it from this video. Yeah, this is a pretty simple setup. We need to feed in our OpenAI key. We need to put in our path here to our JSONL object or file, right? And that is basically it. The purpose is going to be fine-tune. And we have a simply, yeah, we need our file ID here. So it's very important to print that because you need that in the next step when we actually are going to create the model. Remember, this is the step to upload it. So I'm just going to show you on screen now how I did this yesterday because I had to do this in advance. And here you can see what I do here. I just go into this Python script. I run the script here. And just like that, we have our files uploaded. So it's very important now that you copy the file ID and save that because, like I said, we need that in the next step. And that brings us over to step number three. That is going to be create a fine-tuning job. And again, for this, we need a small Python script to run this. You can find this over on OpenAI documentation for fine-tuning. And I have created the script here. So you can see this is where we are going to put in our file ID. So this is quite important. And here is where you select your model you want to fine-tune. I want to use the GPT 3.5 TurboWrite. So this is going to be fine-tuningjob.create from OpenAI. We have the file ID. We have the model name. And we want to print this with a job ID. This job ID we need if we are going to monitor our progress. So we also want to save that. And yeah, that's basically all we have to do. So as you're going to see on the screen here now, this is from yesterday. So I'm just going to run the script again. And just like that, the fine-tuning job is created. So I go ahead. I copy the job ID. I'm going to save that if I want to monitor. This is a very small job, so I don't think we even have to monitor this step. But if you have a big job, this could take some time. Then you just want to go in and see if everything has started and stuff. So keep the file ID and the job ID saved somewhere so you can look it up if something goes wrong, right? Okay, so step number four is actually using your fine-tune model. So how I like to check if the model has completed is just to go to my playground here, right? I just go to playground. I go to chat. I go to model. And here you can see fine-tunes. So this is the model we just trained, right? And now we are actually ready to use this. There are kind of two ways you can use this now. We can either just use it here in the playground, right? That's very simple. We just select the model here and it's ready to go. Remember, here now the only thing we have to put in is kind of just this simple prompt here. We can even shorten this if we kind of wanted to, but it's very short. We don't have to put in any system prompts. Only thing we have to do is just change up kind of the topic here. So if we want a different story, we can just change the topic now. So if we run this now, hopefully we will get a title and a story that kind of fits our data sets we trained this model on, right? So let's try that. And yeah, perfect. You can see we got a title, we got a story, and this is the format we were looking for. And kind of the same story type, the story length. And if we just remove this, run it again, we get a different story. This saves me a lot of time. Like if I just use, let's say, ChatGPT to create the stories, I have to create the story, I have to look for a title. And of course, if we filled in a lot of more examples of this, as I might do some other time, or when GPT-4 gets available, we will get better results. But for now, this is very cheap, it's very fast, and yeah, it works well. But this is only one of the ways we can use this. If we go over to our Python script here, I just have a simple Python script here. So you can see here, we can put in our model name right inside here. If we wanted to try this, there's no point for you for trying this model, it's not gonna work. We can't share this. So there's no point trying to copy this, then we can run it as an API call here, if I just run this now here. And let's say we grab the same prompt as we used here, let me just do this. So let's copy that, and we get back to the terminal here, I paste in this. And then we can get this as an API call, so we can actually use this prompt both in our playground and in our API calls. So here you can see the case of the haunting number, and we get a story. So you can see this works on both of these instances. And that's basically it. Now our model is ready to use. I just wanted to take a quick look at the pricing. So basically, this model cost me 25 cents. So I was quite surprised by that my previous has been the fine tuning is quite expensive. But in this case, it seems very fair. Of course, now I only did a few examples. But we can look at the pricing here. If we go to fine tuning models, GPT 3.5 turbo here, outputs is going to cost you $0.016 per 1000 tokens. Here, it's gonna be like, yeah, it's a bit steeper. It's about, I would say eight times more expensive, but it's still quite low. This is a use case you really need. Yeah, I will probably go for it. At least I'm gonna try to, but I'm waiting for GPT-4. I think that's gonna be much more interesting. But at least it's working well. And I'm quite happy actually how the results turn out. And it's very cheap to run. Just some final words on the GPT 3.5 turbo fine tuning. And yeah, I think it's quite interesting. Actually, I think it's a nice way to get familiar with how you can actually use this in your use cases. Because we see here now with fine tuning for GPT-4 coming this fall, you can already start getting familiar with this if you have like a big plan to use a fine-tuned GPT-4 model. But like you said, early tests have shown that fine-tuned versions of 3.5 turbo can match or even outperform base GPT-4 level capabilities of narrow task. And that is the great thing about a fine-tuned model. So like I showed you over here, this is a very narrow task to be used for. Just creating Instagram stories with our fine-tuned model, that is a very narrow task. I hope this guide gave you some instructions that you actually can use. And maybe you want to try this out. Like I said, all of these python scripts here is going to be uploaded to my members github. So if you want to try that, go in the description below and you can find a link to my membership and sign up and get access to this and a lot of other videos and a more in-depth step-by-step guide on how you can do this. So yeah, I think that was it. Hopefully it was helpful. Give this video a like if it was. Maybe subscribe if you want more of this. Have a great day and hopefully I'll see you in the next one.